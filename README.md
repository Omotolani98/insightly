# Insightly: Time-Series Log Summarizer

Insightly is an intelligent log summarizer that ingests logs via gRPC, batches them in Redis Streams, summarizes each time window using a local LLM (Docker Model Runner), and stores human-readable digests in PostgreSQL. You can query summaries via gRPC or through a simple HTML interface.

## ğŸ“ Folder Structure

```
â”œâ”€â”€ Makefile                     # Commands to build/run components
â”œâ”€â”€ README.md                    # Documentation
â”œâ”€â”€ bin                          # Compiled binaries
â”‚   â”œâ”€â”€ api                      # API binary
â”‚   â”œâ”€â”€ ingest                   # Ingest binary
â”‚   â”œâ”€â”€ query                    # Query binary
â”‚   â””â”€â”€ summarizer               # Summarizer binary
â”œâ”€â”€ cmd                          # Application entrypoints
â”‚   â”œâ”€â”€ api                      # Fiber HTTP API server
â”‚   â”œâ”€â”€ ingest                   # gRPC ingest server
â”‚   â”œâ”€â”€ query                    # gRPC query server
â”‚   â””â”€â”€ summarizer               # Log summarization worker
â”œâ”€â”€ deployments                  # Dockerfiles
â”œâ”€â”€ docker-compose.yml           # Compose file for Redis & PostgreSQL
â”œâ”€â”€ internal                     # Application logic
â”‚   â”œâ”€â”€ cache                    # Redis caching logic
â”‚   â”œâ”€â”€ config                   # Application configuration
â”‚   â”œâ”€â”€ db                       # PostgreSQL database connections
â”‚   â”œâ”€â”€ ingest                   # Ingest handlers
â”‚   â”œâ”€â”€ llm                      # Local LLM integration
â”‚   â”œâ”€â”€ query                    # Query handlers
â”‚   â”œâ”€â”€ storage                  # Database models and migrations
â”‚   â””â”€â”€ summarizer               # Summarization worker logic
â”œâ”€â”€ proto                        # Protobuf definitions
â”‚   â”œâ”€â”€ ingest                   # Compiled ingest protobuf
â”‚   â”œâ”€â”€ ingest.proto             # Ingest protobuf definition
â”‚   â”œâ”€â”€ query                    # Compiled query protobuf
â”‚   â””â”€â”€ query.proto              # Query protobuf definition
â”œâ”€â”€ scripts                      # Migration scripts
â”œâ”€â”€ views                        # HTML views (not currently used)
â””â”€â”€ .env                         # Environment variables
```

## âš™ï¸ Prerequisites

- Go 1.21+
- Docker & Docker Compose
- Redis & PostgreSQL (auto-deployed via Docker Compose)
- Docker Model Runner enabled on Docker Desktop (`ai/llama3.2` model)
- `protoc` and Go plugins (`protoc-gen-go`, `protoc-gen-go-grpc`)

## ğŸ›  Components

| Component             | Role                                                   | Startup           |
|-----------------------|--------------------------------------------------------|-------------------|
| **Ingest API**        | gRPC server: streams logs to Redis                     | `./start.sh`      |
| **Redis Stream**      | Buffers logs temporarily                               | Docker Compose    |
| **Summarizer Worker** | Processes logs with LLM, saves summaries to PostgreSQL | `./start.sh`      |
| **Dockerâ€‘LLM**        | Local LLM container (Docker Model Runner)              | Docker Desktop    |
| **PostgreSQL**        | Stores human-readable summaries                        | Docker Compose    |
| **Query API**         | gRPC server: fetches summaries                         | `./start.sh`      |
| **Fiber API**         | HTTP server for frontend summary fetching              | `./start.sh`      |

## ğŸš€ Running Locally

1. Clone the repository and navigate into the folder:

```bash
git clone [REPO_URL]
cd insightly
```

2. Install dependencies:

```bash
go mod tidy
```

3. Generate Protobuf files:

```bash
protoc --go_out=. --go-grpc_out=. proto/*.proto
```

4. Enable Docker Model Runner:

- Open Docker Desktop â†’ Settings â†’ Features In Development
- Check "Enable Docker Model Runner" & "Enable host-side TCP support"
- Confirm LLM runs at `localhost:12434`

5. Create a `.env` file:

```env
# PostgreSQL
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=insightly
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Redis
REDIS_HOST=redis
REDIS_PORT=6379

# LLM
LLM_HOST=localhost
LLM_PORT=12434
ENGINE_ID=llama.cpp
MODEL_NAME=ai/llama3.2

# gRPC Ports
INGEST_PORT=50051
QUERY_PORT=50052
```

6. Start the Insightly stack:

```bash
./start.sh
```

- To stop the application, use:

```bash
make down
```

## ğŸ“¡ Testing with Postman

- Open Postman
- Create a gRPC request
  - Upload Proto file (`proto/ingest.proto`)
  - Select the `StreamLogs` method
- Send the message

Check query logs using another gRPC request (`proto/query.proto`) with the `GetSummaries` method.

## ğŸŒ Viewing Summaries

Create a simple frontend using HTML, Tailwind CSS, and JavaScript:

### `index.html`

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Insightly</title>
  <script defer src="app.js"></script>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel="stylesheet">
  <style>
    body { font-family: 'Inter', sans-serif; }
  </style>
</head>
<body class="bg-gray-50 p-8">
  <div class="max-w-2xl mx-auto bg-white rounded-lg shadow p-6">
    <h1 class="text-2xl font-bold mb-4">Insightly</h1>
    <input id="filterInput" type="text" placeholder="Filter by stream..." class="border p-2 rounded w-full">
    <ul id="summaryList" class="space-y-4 mt-4"></ul>
  </div>
</body>
</html>
```

### `app.js`

[JS content remains unchanged from previous version]

